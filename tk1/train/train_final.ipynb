{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Complete Pipeline + Enhanced MobileNetV3-Small\n",
    "\n",
    " Full pipeline with all visualizations + CBAM, Enhanced Head, CutMix, Progressive Resizing, TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, opencv-python\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [opencv-python]0m [opencv-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python\n",
    "pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bbsw/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ===== IMPORTS =====\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# ===== IMPORTS =====\n",
    "import os, shutil, random, cv2, torch, gc, time, subprocess, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "import cv2\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from torch.amp import GradScaler, autocast\n",
    "    _NEW_AMP = True\n",
    "except:\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    _NEW_AMP = False\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "CONFIG = {\n",
    "    # Model Enhancements\n",
    "    'use_cbam': True,                    # CBAM attention module\n",
    "    'use_better_head': True,             # Enhanced classification head\n",
    "    \n",
    "    # Training\n",
    "    'img_size': 224,  # Start small and progressively increase\n",
    "    'batch_size': 256,\n",
    "    'epochs': 20,\n",
    "    'lr': 2e-4,  # Slightly increased learning rate\n",
    "    \n",
    "    # Progressive Resizing - increase only, within training epochs\n",
    "    'use_progressive_resize': True,\n",
    "    'progressive_schedule': {\n",
    "        1: 224,\n",
    "        5: 256,\n",
    "        10: 288,\n",
    "        15: 320\n",
    "    },\n",
    "    \n",
    "    # Augmentation\n",
    "    'use_mixup': True,\n",
    "    'use_cutmix': True,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'cutmix_alpha': 1.0,\n",
    "    'use_label_smoothing': True,\n",
    "    'label_smooth_eps': 0.1,\n",
    "    \n",
    "    # Inference\n",
    "    'use_tta': True,\n",
    "    'tta_transforms': 5,\n",
    "    \n",
    "    # YOLO\n",
    "    'yolo_epochs': 20,\n",
    "    'yolo_imgsz': 640,\n",
    "    'yolo_batch': 16,\n",
    "    'yolo_conf': 0.25,\n",
    "    \n",
    "    # Splits\n",
    "    'val_size': 0.15,\n",
    "    'test_size': 0.15,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_dir\n\u001b[0;32m----> 9\u001b[0m PATH_OUTPUT \u001b[38;5;241m=\u001b[39m \u001b[43mget_output_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGK-final\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_output_structure\u001b[39m(base_path):\n\u001b[1;32m     12\u001b[0m     folders \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_images\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrops\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexports\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mget_output_folder\u001b[0;34m(parent_dir, env_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_output_folder\u001b[39m(parent_dir: \u001b[38;5;28mstr\u001b[39m, env_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(parent_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     experiment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_dir, experiment_id)\n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== SETUP =====\n",
    "def get_output_folder(parent_dir: str, env_name: str) -> str:\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    experiment_id = f\"{env_name}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    output_dir = os.path.join(parent_dir, experiment_id)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "PATH_OUTPUT = get_output_folder(\"../output\", \"GK-final\")\n",
    "\n",
    "def create_output_structure(base_path):\n",
    "    folders = [\"field_images\", \"yolo_weights\", \"crops\", \"crop_samples\",\n",
    "               \"weights\", \"results\", \"plots\", \"logs\", \"exports\", \"demo\"]\n",
    "    for folder in folders:\n",
    "        os.makedirs(os.path.join(base_path, folder), exist_ok=True)\n",
    "    return {folder: os.path.join(base_path, folder) for folder in folders}\n",
    "\n",
    "OUTPUT_DIRS = create_output_structure(PATH_OUTPUT)\n",
    "\n",
    "def setup_logging(output_path):\n",
    "    log_file = os.path.join(output_path, \"logs\", \"pipeline.log\")\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging(PATH_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LABELS =====\n",
    "LABELS = {\n",
    "    0: {\"name\": \"brown_spot\", \"match_substrings\": [\n",
    "        \"../data/rice-disease-dataset/Rice_Leaf_AUG/Brown Spot\",\n",
    "        \"../data/rice-leaf-disease-image/Brownspot\",\n",
    "        \"../data/rice-leaf-diseases/rice_leaf_diseases/Brown spot\",\n",
    "        \"../data/rice-leafs-disease-dataset/RiceLeafsDisease/train/brown_spot\",\n",
    "        \"../data/rice-leaf-images/rice_images/_BrownSpot\",\n",
    "        \"../data/rice-diseases-image-dataset/RiceDiseaseDataset/train/BrownSpot\",\n",
    "    ]},\n",
    "    1: {\"name\": \"leaf_blast\", \"match_substrings\": [\n",
    "        \"../data/rice-disease-dataset/Rice_Leaf_AUG/Leaf Blast\",\n",
    "        \"../data/rice-leafs-disease-dataset/RiceLeafsDisease/train/leaf_blast\",\n",
    "        \"../data/rice-leaf-images/rice_images/_LeafBlast\",\n",
    "        \"../data/rice-diseases-image-dataset/RiceDiseaseDataset/train/LeafBlast\",\n",
    "    ]},\n",
    "    2: {\"name\": \"leaf_blight\", \"match_substrings\": [\n",
    "        \"../data/rice-disease-dataset/Rice_Leaf_AUG/Sheath Blight\",\n",
    "        \"../data/rice-leaf-diseases/rice_leaf_diseases/Bacterial leaf blight\",\n",
    "        \"../data/rice-leaf-disease-image/Bacterialblight\",\n",
    "        \"../data/rice-leafs-disease-dataset/RiceLeafsDisease/train/bacterial_leaf_blight\",\n",
    "    ]},\n",
    "    3: {\"name\": \"healthy\", \"match_substrings\": [\n",
    "        \"../data/rice-disease-dataset/Rice_Leaf_AUG/Healthy Rice Leaf\",\n",
    "        \"../data/rice-leafs-disease-dataset/RiceLeafsDisease/train/healthy\",\n",
    "        \"../data/rice-leaf-images/rice_images/_Healthy\",\n",
    "        \"../data/rice-diseases-image-dataset/RiceDiseaseDataset/train/Healthy\",\n",
    "    ]}\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INSTALL ULTRALYTICS =====\n",
    "def install_ultralytics():\n",
    "    try:\n",
    "        import ultralytics\n",
    "        logging.info(\"Ultralytics already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        logging.info(\"Installing ultralytics...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n",
    "        return True\n",
    "\n",
    "YOLO_AVAILABLE = install_ultralytics()\n",
    "if YOLO_AVAILABLE:\n",
    "    from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ENHANCED MODEL COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CBAM ATTENTION =====\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        out = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n",
    "        return x * out\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.sigmoid(self.conv(out))\n",
    "        return x * out\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_att = SpatialAttention()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        x = self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "# ===== ENHANCED HEAD =====\n",
    "class EnhancedHead(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        hidden = in_features // 2\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features * 2, hidden),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x).flatten(1)\n",
    "        max_out = self.max_pool(x).flatten(1)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ===== ENHANCED MOBILENETV3 =====\n",
    "def build_enhanced_mobilenetv3(num_classes):\n",
    "    from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "    \n",
    "    weights = MobileNet_V3_Small_Weights.DEFAULT\n",
    "    model = mobilenet_v3_small(weights=weights)\n",
    "    \n",
    "    if CONFIG['use_cbam']:\n",
    "        in_channels = model.features[-1][0].out_channels\n",
    "        cbam = CBAM(in_channels, reduction=16)\n",
    "        original_features = model.features\n",
    "        model.features = nn.Sequential(*list(original_features.children()), cbam)\n",
    "        logging.info(\"✓ Added CBAM attention\")\n",
    "    \n",
    "    if CONFIG['use_better_head']:\n",
    "        in_features = model.classifier[0].in_features\n",
    "        model.classifier = nn.Identity()\n",
    "        model.enhanced_head = EnhancedHead(in_features, num_classes, dropout=0.3)\n",
    "        \n",
    "        original_forward = model.forward\n",
    "        def new_forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = self.enhanced_head(x)\n",
    "            return x\n",
    "        model.forward = new_forward.__get__(model, type(model))\n",
    "        logging.info(\"✓ Added enhanced head\")\n",
    "    else:\n",
    "        in_features = model.classifier[3].in_features\n",
    "        model.classifier[3] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images_from_path(path: str) -> List[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')\n",
    "    images = []\n",
    "    try:\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(image_extensions):\n",
    "                images.append(os.path.join(path, file))\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error reading {path}: {e}\")\n",
    "    return images\n",
    "\n",
    "def auto_collect_dataset():\n",
    "    logging.info(\"=\"*60)\n",
    "    logging.info(\"DATA COLLECTION\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    all_data = []\n",
    "    for label_id, label_info in LABELS.items():\n",
    "        label_name = label_info['name']\n",
    "        match_paths = label_info['match_substrings']\n",
    "        \n",
    "        logging.info(f\"\\nCollecting {label_name} (ID: {label_id})...\")\n",
    "        \n",
    "        for path in match_paths:\n",
    "            images = collect_images_from_path(path)\n",
    "            if len(images) > 0:\n",
    "                logging.info(f\"  ✓ {len(images)} images from {path}\")\n",
    "                for img_path in images:\n",
    "                    all_data.append({\n",
    "                        'image_path': img_path,\n",
    "                        'label_id': label_id,\n",
    "                        'label_name': label_name,\n",
    "                        'source_path': path\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    logging.info(f\"\\nTotal: {len(df)} images\")\n",
    "    logging.info(f\"\\nBy label:\\n{df.groupby('label_name').size()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "collected_df = auto_collect_dataset()\n",
    "collected_df.to_csv(os.path.join(OUTPUT_DIRS[\"results\"], \"collected_images.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 2: Prepare Dataset (Smart Labeling for Single Leaf vs Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image_type(img_path, edge_threshold=0.15):\n",
    "    \"\"\"\n",
    "    Detect if image is single leaf or rice plant cluster\n",
    "    \n",
    "    Strategy:\n",
    "    - Single leaf: Usually centered, edges are clear background\n",
    "    - Cluster/plant: Complex, multiple objects, edges have content\n",
    "    \n",
    "    Returns: 'single_leaf' or 'cluster'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return 'unknown'\n",
    "        \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        h, w = gray.shape\n",
    "        \n",
    "        # Check edge density (if edges have little content → single leaf)\n",
    "        edge_width = int(w * 0.1)  # 10% from each edge\n",
    "        edge_height = int(h * 0.1)\n",
    "        \n",
    "        # Extract edge regions\n",
    "        top_edge = gray[:edge_height, :]\n",
    "        bottom_edge = gray[h-edge_height:, :]\n",
    "        left_edge = gray[:, :edge_width]\n",
    "        right_edge = gray[:, w-edge_width:]\n",
    "        \n",
    "        # Calculate edge density using standard deviation\n",
    "        # Low std → uniform background → single leaf\n",
    "        # High std → complex content → cluster\n",
    "        edge_std = np.mean([\n",
    "            np.std(top_edge),\n",
    "            np.std(bottom_edge),\n",
    "            np.std(left_edge),\n",
    "            np.std(right_edge)\n",
    "        ])\n",
    "        \n",
    "        center_std = np.std(gray[edge_height:h-edge_height, edge_width:w-edge_width])\n",
    "        \n",
    "        # If edge is much simpler than center → single leaf\n",
    "        if center_std > 0:\n",
    "            edge_ratio = edge_std / center_std\n",
    "            if edge_ratio < edge_threshold:\n",
    "                return 'single_leaf'\n",
    "        \n",
    "        return 'cluster'\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error detecting type for {img_path}: {e}\")\n",
    "        return 'unknown'\n",
    "\n",
    "def create_pseudo_labels_for_cluster(img_path, label_id):\n",
    "    \"\"\"\n",
    "    For cluster images, create pseudo bounding boxes using image processing\n",
    "    \n",
    "    Strategy:\n",
    "    1. Use color-based segmentation to find green regions (leaves)\n",
    "    2. Find contours and create bounding boxes\n",
    "    3. Filter small/noisy detections\n",
    "    \n",
    "    Returns: List of (class_id, x_center, y_center, width, height) normalized\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return []\n",
    "        \n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img_rgb.shape[:2]\n",
    "        \n",
    "        # Convert to HSV for better green detection\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Define range for green color (leaves)\n",
    "        lower_green = np.array([25, 30, 30])\n",
    "        upper_green = np.array([90, 255, 255])\n",
    "        \n",
    "        mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "        \n",
    "        # Morphological operations to clean up\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        bboxes = []\n",
    "        min_area = (w * h) * 0.01  # At least 1% of image\n",
    "        \n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area < min_area:\n",
    "                continue\n",
    "            \n",
    "            x, y, box_w, box_h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Skip very thin boxes (noise)\n",
    "            aspect_ratio = box_w / box_h if box_h > 0 else 0\n",
    "            if aspect_ratio < 0.1 or aspect_ratio > 10:\n",
    "                continue\n",
    "            \n",
    "            # Normalize to YOLO format\n",
    "            x_center = (x + box_w / 2) / w\n",
    "            y_center = (y + box_h / 2) / h\n",
    "            norm_w = box_w / w\n",
    "            norm_h = box_h / h\n",
    "            \n",
    "            bboxes.append((label_id, x_center, y_center, norm_w, norm_h))\n",
    "        \n",
    "        # If no boxes found, fall back to full image\n",
    "        if len(bboxes) == 0:\n",
    "            bboxes = [(label_id, 0.5, 0.5, 1.0, 1.0)]\n",
    "        \n",
    "        return bboxes\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error creating pseudo labels for {img_path}: {e}\")\n",
    "        return [(label_id, 0.5, 0.5, 1.0, 1.0)]  # Fallback\n",
    "\n",
    "def prepare_field_dataset(df, output_dir, val_size=0.15, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Prepare dataset with SMART LABELING:\n",
    "    - Single leaf images: Use full-image bbox (valid assumption)\n",
    "    - Cluster images: Use pseudo-labels from image processing\n",
    "    \"\"\"\n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"SMART DATASET PREPARATION\")\n",
    "    logging.info(\"=\"*60)\n",
    "    logging.info(\"Strategy:\")\n",
    "    logging.info(\"  • Single leaf images → Full-image bbox\")\n",
    "    logging.info(\"  • Cluster images → Pseudo-labels from segmentation\")\n",
    "    \n",
    "    trainval_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=42, stratify=df['label_id']\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        trainval_df, test_size=val_size/(1-test_size), random_state=42,\n",
    "        stratify=trainval_df['label_id']\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"\\nTrain: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    logging.info(f\"Val:   {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    logging.info(f\"Test:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    field_data = {'train': [], 'val': [], 'test': []}\n",
    "    type_stats = {'single_leaf': 0, 'cluster': 0, 'unknown': 0}\n",
    "    \n",
    "    for split, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "        split_dir = os.path.join(output_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        logging.info(f\"\\nProcessing {split}...\")\n",
    "        \n",
    "        for idx, row in tqdm(split_df.iterrows(), total=len(split_df), desc=f\"Preparing {split}\"):\n",
    "            src = row['image_path']\n",
    "            dst = os.path.join(split_dir, f\"{split}_{idx:06d}.jpg\")\n",
    "            \n",
    "            try:\n",
    "                shutil.copy2(src, dst)\n",
    "                \n",
    "                # Detect image type\n",
    "                img_type = detect_image_type(src)\n",
    "                type_stats[img_type] = type_stats.get(img_type, 0) + 1\n",
    "                \n",
    "                label_file = os.path.join(split_dir, f\"{split}_{idx:06d}.txt\")\n",
    "                \n",
    "                if img_type == 'single_leaf':\n",
    "                    # Single leaf: Use full-image bbox (valid for centered single leaf)\n",
    "                    with open(label_file, 'w') as f:\n",
    "                        f.write(f\"{row['label_id']} 0.5 0.5 1.0 1.0\\n\")\n",
    "                else:\n",
    "                    # Cluster or unknown: Use pseudo-labels\n",
    "                    bboxes = create_pseudo_labels_for_cluster(src, row['label_id'])\n",
    "                    with open(label_file, 'w') as f:\n",
    "                        for bbox in bboxes:\n",
    "                            class_id, x_c, y_c, w, h = bbox\n",
    "                            f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "                \n",
    "                field_data[split].append({\n",
    "                    'image_path': dst,\n",
    "                    'label_path': label_file,\n",
    "                    'label_name': row['label_name'],\n",
    "                    'label_id': row['label_id'],\n",
    "                    'image_type': img_type\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error: {e}\")\n",
    "    \n",
    "    logging.info(f\"\\n{'='*60}\")\n",
    "    logging.info(\"Image Type Statistics:\")\n",
    "    logging.info(f\"  Single Leaf: {type_stats.get('single_leaf', 0)} \"\n",
    "                f\"({type_stats.get('single_leaf', 0)/len(df)*100:.1f}%)\")\n",
    "    logging.info(f\"  Cluster:     {type_stats.get('cluster', 0)} \"\n",
    "                f\"({type_stats.get('cluster', 0)/len(df)*100:.1f}%)\")\n",
    "    logging.info(f\"  Unknown:     {type_stats.get('unknown', 0)} \"\n",
    "                f\"({type_stats.get('unknown', 0)/len(df)*100:.1f}%)\")\n",
    "    logging.info(f\"{'='*60}\")\n",
    "    \n",
    "    # Save statistics\n",
    "    stats_df = pd.DataFrame([type_stats])\n",
    "    stats_df.to_csv(os.path.join(output_dir, 'image_type_stats.csv'), index=False)\n",
    "    \n",
    "    return field_data\n",
    "\n",
    "def create_yolo_yaml(data_root, output_path):\n",
    "    import yaml\n",
    "    yaml_data = {\n",
    "        'path': str(Path(data_root).absolute()),\n",
    "        'train': 'train', 'val': 'val', 'test': 'test',\n",
    "        'nc': len(LABELS),\n",
    "        'names': [LABELS[i]['name'] for i in sorted(LABELS.keys())]\n",
    "    }\n",
    "    yaml_path = os.path.join(output_path, \"data.yaml\")\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_data, f, default_flow_style=False)\n",
    "    logging.info(f\"Created data.yaml: {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "field_data = prepare_field_dataset(\n",
    "    collected_df, OUTPUT_DIRS[\"field_images\"], \n",
    "    val_size=CONFIG['val_size'], test_size=CONFIG['test_size']\n",
    ")\n",
    "yaml_path = create_yolo_yaml(OUTPUT_DIRS[\"field_images\"], OUTPUT_DIRS[\"field_images\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 3: YOLO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo_detector(yaml_path, output_dir, epochs=30, imgsz=640, batch=16):\n",
    "    if not YOLO_AVAILABLE:\n",
    "        logging.error(\"YOLO not available\")\n",
    "        return None\n",
    "    \n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"YOLO TRAINING\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    yolo_device = '0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    results = model.train(\n",
    "        data=yaml_path, epochs=epochs, imgsz=imgsz, batch=batch,\n",
    "        device=yolo_device, patience=10, save_period=5, workers=4,\n",
    "        project=output_dir, name='detector', exist_ok=True, verbose=True, plots=True\n",
    "    )\n",
    "    \n",
    "    best_model = Path(output_dir) / 'detector' / 'weights' / 'best.pt'\n",
    "    logging.info(f\"✓ Best YOLO model: {best_model}\")\n",
    "    return str(best_model)\n",
    "\n",
    "best_yolo_model = train_yolo_detector(\n",
    "    yaml_path=yaml_path, output_dir=OUTPUT_DIRS[\"yolo_weights\"],\n",
    "    epochs=CONFIG['yolo_epochs'], imgsz=CONFIG['yolo_imgsz'], batch=CONFIG['yolo_batch']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 4: Extract Crops with Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_crop_samples(model, field_images, output_dir, n_samples=6):\n",
    "    \"\"\"Visualize crop extraction: Original → Crops\"\"\"\n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"CREATING CROP VISUALIZATION SAMPLES\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    samples_dir = os.path.join(output_dir, \"crop_samples\")\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    \n",
    "    train_samples = random.sample(field_images['train'], min(n_samples, len(field_images['train'])))\n",
    "    \n",
    "    for sample_idx, item in enumerate(train_samples):\n",
    "        img_path = item['image_path']\n",
    "        parent_label = item['label_name']\n",
    "        \n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            results = model.predict(img_path, conf=0.25, verbose=False)\n",
    "            \n",
    "            crops = []\n",
    "            img_with_boxes = img_rgb.copy()\n",
    "            \n",
    "            for idx, result in enumerate(results[0].boxes):\n",
    "                x1, y1, x2, y2 = map(int, result.xyxy[0].cpu().numpy())\n",
    "                cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "                cv2.putText(img_with_boxes, f\"Crop {idx+1}\", (x1, y1-10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                padding = 10\n",
    "                h, w = img_rgb.shape[:2]\n",
    "                x1_p = max(0, x1 - padding)\n",
    "                y1_p = max(0, y1 - padding)\n",
    "                x2_p = min(w, x2 + padding)\n",
    "                y2_p = min(h, y2 + padding)\n",
    "                \n",
    "                crop = img_rgb[y1_p:y2_p, x1_p:x2_p]\n",
    "                if crop.size > 0 and crop.shape[0] >= 50 and crop.shape[1] >= 50:\n",
    "                    crops.append(crop)\n",
    "            \n",
    "            if len(crops) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_crops = len(crops)\n",
    "            fig = plt.figure(figsize=(16, 10))\n",
    "            \n",
    "            ax = plt.subplot(2, 4, (1, 5))\n",
    "            ax.imshow(img_with_boxes)\n",
    "            ax.set_title(f'Original Image (Label: {parent_label})\\n{n_crops} crops detected', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            for i, crop in enumerate(crops[:6]):\n",
    "                ax = plt.subplot(2, 4, i+2 if i < 3 else i+3)\n",
    "                ax.imshow(crop)\n",
    "                ax.set_title(f'Crop {i+1}\\n(Inherited: {parent_label})', fontsize=10)\n",
    "                ax.axis('off')\n",
    "            \n",
    "            plt.suptitle(f'Sample {sample_idx+1}: Crop Extraction with Inheritance Labeling', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            save_path = os.path.join(samples_dir, f'crop_sample_{sample_idx+1}.png')\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            logging.info(f\"  ✓ Saved sample {sample_idx+1}: {n_crops} crops from {parent_label}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error creating sample {sample_idx+1}: {e}\")\n",
    "\n",
    "def extract_crops_with_inheritance_labeling(yolo_model_path, field_images, output_dir, confidence=0.25):\n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"EXTRACTING CROPS WITH INHERITANCE LABELING\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    model = YOLO(yolo_model_path)\n",
    "    crops_data = []\n",
    "    \n",
    "    # First, create visualization samples\n",
    "    visualize_crop_samples(model, field_images, output_dir, n_samples=6)\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = os.path.join(output_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        logging.info(f\"\\nProcessing {split}...\")\n",
    "        \n",
    "        for item in tqdm(field_images[split], desc=f\"Extracting {split}\"):\n",
    "            img_path = item['image_path']\n",
    "            parent_label_id = item['label_id']\n",
    "            parent_label_name = item['label_name']\n",
    "            \n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                results = model.predict(img_path, conf=confidence, verbose=False)\n",
    "                base_name = Path(img_path).stem\n",
    "                \n",
    "                for idx, result in enumerate(results[0].boxes):\n",
    "                    x1, y1, x2, y2 = map(int, result.xyxy[0].cpu().numpy())\n",
    "                    \n",
    "                    padding = 10\n",
    "                    h, w = img_rgb.shape[:2]\n",
    "                    x1 = max(0, x1 - padding)\n",
    "                    y1 = max(0, y1 - padding)\n",
    "                    x2 = min(w, x2 + padding)\n",
    "                    y2 = min(h, y2 + padding)\n",
    "                    \n",
    "                    crop = img_rgb[y1:y2, x1:x2]\n",
    "                    \n",
    "                    if crop.size == 0 or crop.shape[0] < 50 or crop.shape[1] < 50:\n",
    "                        continue\n",
    "                    \n",
    "                    crop_filename = f\"{base_name}_crop{idx:03d}.jpg\"\n",
    "                    crop_path = os.path.join(split_dir, crop_filename)\n",
    "                    Image.fromarray(crop).save(crop_path)\n",
    "                    \n",
    "                    crops_data.append({\n",
    "                        'crop_path': crop_path,\n",
    "                        'parent_image': img_path,\n",
    "                        'split': split,\n",
    "                        'label_id': parent_label_id,\n",
    "                        'label_name': parent_label_name,\n",
    "                        'crop_id': f\"{base_name}_crop{idx:03d}\"\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error: {e}\")\n",
    "    \n",
    "    crops_df = pd.DataFrame(crops_data)\n",
    "    crops_csv = os.path.join(output_dir, \"crops_metadata.csv\")\n",
    "    crops_df.to_csv(crops_csv, index=False)\n",
    "    \n",
    "    logging.info(f\"\\n✓ Extracted {len(crops_df)} crops with inherited labels\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        count = len(crops_df[crops_df['split']==split])\n",
    "        logging.info(f\"  {split}: {count}\")\n",
    "    logging.info(f\"\\nLabel distribution:\\n{crops_df.groupby(['split', 'label_name']).size()}\")\n",
    "    \n",
    "    return crops_df\n",
    "\n",
    "crops_df = extract_crops_with_inheritance_labeling(\n",
    "    yolo_model_path=best_yolo_model,\n",
    "    field_images=field_data,\n",
    "    output_dir=OUTPUT_DIRS[\"crops\"],\n",
    "    confidence=CONFIG['yolo_conf']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 5: Data Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_analysis_report(collected_df, crops_df, output_dir):\n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"CREATING DATA ANALYSIS REPORT\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Original images distribution\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    label_counts = collected_df.groupby('label_name').size().sort_values(ascending=True)\n",
    "    colors = plt.cm.Set3(range(len(label_counts)))\n",
    "    label_counts.plot(kind='barh', ax=ax1, color=colors)\n",
    "    ax1.set_xlabel('Number of Images', fontsize=10)\n",
    "    ax1.set_title('Original Images per Disease Class', fontsize=11, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 2. Crops distribution\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    crop_counts = crops_df.groupby('label_name').size().sort_values(ascending=True)\n",
    "    crop_counts.plot(kind='barh', ax=ax2, color=colors)\n",
    "    ax2.set_xlabel('Number of Crops', fontsize=10)\n",
    "    ax2.set_title('Extracted Crops per Disease Class', fontsize=11, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 3. Crops per split\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    split_counts = crops_df.groupby('split').size()\n",
    "    split_colors = ['steelblue', 'coral', 'lightgreen']\n",
    "    ax3.bar(split_counts.index, split_counts.values, color=split_colors)\n",
    "    ax3.set_ylabel('Number of Crops', fontsize=10)\n",
    "    ax3.set_title('Crops per Split', fontsize=11, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    for i, v in enumerate(split_counts.values):\n",
    "        ax3.text(i, v, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Distribution by split and class\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    pivot = crops_df.groupby(['label_name', 'split']).size().unstack(fill_value=0)\n",
    "    pivot.plot(kind='bar', stacked=True, ax=ax4, color=split_colors)\n",
    "    ax4.set_xlabel('Disease Class', fontsize=10)\n",
    "    ax4.set_ylabel('Number of Crops', fontsize=10)\n",
    "    ax4.set_title('Class Distribution Across Splits', fontsize=11, fontweight='bold')\n",
    "    ax4.legend(title='Split', loc='upper right')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 5. Pie chart\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    overall_counts = crops_df.groupby('label_name').size()\n",
    "    ax5.pie(overall_counts.values, labels=overall_counts.index, autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90)\n",
    "    ax5.set_title('Overall Crop Distribution', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 6. Train/Val/Test ratio\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    split_pct = crops_df.groupby('split').size() / len(crops_df) * 100\n",
    "    ax6.pie(split_pct.values, labels=[f'{s}\\n({v:.1f}%)' for s, v in split_pct.items()],\n",
    "           colors=split_colors, startangle=90)\n",
    "    ax6.set_title('Train/Val/Test Split Ratio', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 7. Crops per parent image stats\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    crops_per_parent = crops_df.groupby('parent_image').size()\n",
    "    ax7.hist(crops_per_parent.values, bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax7.axvline(crops_per_parent.mean(), color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {crops_per_parent.mean():.1f}')\n",
    "    ax7.set_xlabel('Crops per Parent Image', fontsize=10)\n",
    "    ax7.set_ylabel('Frequency', fontsize=10)\n",
    "    ax7.set_title('Crops Extracted per Parent Image', fontsize=11, fontweight='bold')\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 8. Class balance\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    train_dist = crops_df[crops_df['split']=='train'].groupby('label_name').size()\n",
    "    val_dist = crops_df[crops_df['split']=='val'].groupby('label_name').size()\n",
    "    test_dist = crops_df[crops_df['split']=='test'].groupby('label_name').size()\n",
    "    \n",
    "    x = np.arange(len(train_dist))\n",
    "    width = 0.25\n",
    "    ax8.bar(x - width, train_dist.values, width, label='Train', color='steelblue')\n",
    "    ax8.bar(x, val_dist.values, width, label='Val', color='coral')\n",
    "    ax8.bar(x + width, test_dist.values, width, label='Test', color='lightgreen')\n",
    "    \n",
    "    ax8.set_xlabel('Disease Class', fontsize=10)\n",
    "    ax8.set_ylabel('Number of Samples', fontsize=10)\n",
    "    ax8.set_title('Class Balance Across Splits', fontsize=11, fontweight='bold')\n",
    "    ax8.set_xticks(x)\n",
    "    ax8.set_xticklabels(train_dist.index, rotation=45, ha='right')\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 9. Summary\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    ax9.axis('off')\n",
    "    stats_text = f\"\"\"\n",
    "    DATA SUMMARY STATISTICS\n",
    "    ═══════════════════════════\n",
    "    \n",
    "    Original Images: {len(collected_df):,}\n",
    "    Total Crops: {len(crops_df):,}\n",
    "    Classes: {len(crops_df['label_name'].unique())}\n",
    "    \n",
    "    Split Distribution:\n",
    "      • Train: {len(crops_df[crops_df['split']=='train']):,} ({len(crops_df[crops_df['split']=='train'])/len(crops_df)*100:.1f}%)\n",
    "      • Val:   {len(crops_df[crops_df['split']=='val']):,} ({len(crops_df[crops_df['split']=='val'])/len(crops_df)*100:.1f}%)\n",
    "      • Test:  {len(crops_df[crops_df['split']=='test']):,} ({len(crops_df[crops_df['split']=='test'])/len(crops_df)*100:.1f}%)\n",
    "    \n",
    "    Avg Crops/Image: {crops_per_parent.mean():.2f}\n",
    "    Min: {crops_per_parent.min()}\n",
    "    Max: {crops_per_parent.max()}\n",
    "    \n",
    "    Enhancements:\n",
    "      CBAM: {CONFIG['use_cbam']}\n",
    "      Enhanced Head: {CONFIG['use_better_head']}\n",
    "      CutMix: {CONFIG['use_cutmix']}\n",
    "      Progressive: {CONFIG['use_progressive_resize']}\n",
    "      TTA: {CONFIG['use_tta']}\n",
    "    \"\"\"\n",
    "    ax9.text(0.1, 0.5, stats_text, fontsize=10, family='monospace',\n",
    "            verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.suptitle('Comprehensive Data Analysis Report', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(output_dir, \"data_analysis_report.png\")\n",
    "    plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logging.info(f\"✓ Data analysis report saved: {save_path}\")\n",
    "\n",
    "create_data_analysis_report(collected_df, crops_df, OUTPUT_DIRS[\"plots\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 6: Enhanced Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CUTMIX =====\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size, device=images.device)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    \n",
    "    _, _, h, w = images.size()\n",
    "    cut_rat = np.sqrt(1.0 - lam)\n",
    "    cut_w = int(w * cut_rat)\n",
    "    cut_h = int(h * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(w)\n",
    "    cy = np.random.randint(h)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "    \n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (w * h))\n",
    "    \n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "# ===== MIXUP =====\n",
    "class MixUpDataset(Dataset):\n",
    "    def __init__(self, base_dataset, alpha=0.2):\n",
    "        self.dataset = base_dataset\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1, label1 = self.dataset[idx]\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            idx2 = random.randint(0, len(self.dataset) - 1)\n",
    "            img2, label2 = self.dataset[idx2]\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            mixed_img = lam * img1 + (1 - lam) * img2\n",
    "            return mixed_img, label1, label2, lam\n",
    "        else:\n",
    "            return img1, label1, label1, 1.0\n",
    "\n",
    "# ===== LABEL SMOOTHING =====\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_preds = F.log_softmax(pred, dim=-1)\n",
    "        loss = -log_preds.sum(dim=-1).mean()\n",
    "        nll = F.nll_loss(log_preds, target)\n",
    "        return (1 - self.epsilon) * nll + self.epsilon * (loss / n_classes)\n",
    "\n",
    "# ===== DATASET =====\n",
    "class CropDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"crop_path\"]).convert(\"RGB\")\n",
    "        label = int(row[\"label_id\"])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ===== PROGRESSIVE TRANSFORMS =====\n",
    "def get_transforms(size):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 7: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_amp():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if not use_cuda:\n",
    "        from contextlib import nullcontext\n",
    "        amp_ctx = nullcontext()\n",
    "    else:\n",
    "        if _NEW_AMP:\n",
    "            amp_ctx = autocast(device_type=\"cuda\", enabled=True)\n",
    "        else:\n",
    "            amp_ctx = autocast(enabled=True)\n",
    "    \n",
    "    if _NEW_AMP:\n",
    "        scaler = GradScaler(device=\"cuda\" if use_cuda else \"cpu\", enabled=use_cuda)\n",
    "    else:\n",
    "        scaler = GradScaler(enabled=use_cuda)\n",
    "    \n",
    "    return DEVICE, amp_ctx, scaler\n",
    "\n",
    "def train_enhanced_model(model, train_df, val_df, epochs):\n",
    "    device, amp_ctx, scaler = setup_amp()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if CONFIG['use_label_smoothing']:\n",
    "        criterion = LabelSmoothingCrossEntropy(CONFIG['label_smooth_eps'])\n",
    "        logging.info(\"Using Label Smoothing\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_path = os.path.join(OUTPUT_DIRS[\"weights\"], \"enhanced_mobilenetv3_best.pth\")\n",
    "    \n",
    "    current_size = CONFIG['img_size']\n",
    "    \n",
    "    logging.info(f\"\\nTraining Enhanced MobileNetV3-Small...\")\n",
    "    logging.info(f\"Enhancements: CBAM={CONFIG['use_cbam']}, Head={CONFIG['use_better_head']}, \"\n",
    "                f\"CutMix={CONFIG['use_cutmix']}, Progressive={CONFIG['use_progressive_resize']}\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Progressive resizing\n",
    "        if CONFIG['use_progressive_resize']:\n",
    "            if epoch in CONFIG['progressive_schedule']:\n",
    "                current_size = CONFIG['progressive_schedule'][epoch]\n",
    "                logging.info(f\"\\n→ Progressive resize to {current_size}x{current_size}\")\n",
    "        \n",
    "        # Check if we should disable strong augmentation for fine-tuning\n",
    "        use_strong_aug = epoch <= int(0.8 * epochs)\n",
    "        if epoch == int(0.8 * epochs) + 1:\n",
    "            logging.info(f\"\\n→ Disabling MixUp/CutMix for fine-tuning (epoch {epoch}/{epochs})\")\n",
    "        \n",
    "        # Update transforms and loaders\n",
    "        train_transform, val_transform = get_transforms(current_size)\n",
    "        \n",
    "        if CONFIG['use_mixup']:\n",
    "            train_dataset_base = CropDataset(train_df, transform=train_transform)\n",
    "            train_dataset = MixUpDataset(train_dataset_base, alpha=CONFIG['mixup_alpha'])\n",
    "        else:\n",
    "            train_dataset = CropDataset(train_df, transform=train_transform)\n",
    "        \n",
    "        val_dataset = CropDataset(val_df, transform=val_transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                                 shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                               shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            # Check if we should use strong augmentation (disable in last 20% of epochs)\n",
    "            use_strong_aug = epoch <= int(0.8 * epochs)\n",
    "            enable_mixup = CONFIG['use_mixup'] and use_strong_aug\n",
    "            enable_cutmix = CONFIG['use_cutmix'] and use_strong_aug\n",
    "            \n",
    "            if enable_mixup and len(batch) == 4:  # MixUp\n",
    "                imgs, labels1, labels2, lam = batch\n",
    "                imgs = imgs.to(device)\n",
    "                labels1 = labels1.long().to(device)\n",
    "                labels2 = labels2.long().to(device)\n",
    "                \n",
    "                # lam should be tensor per batch [B] for per-sample mixing\n",
    "                if not torch.is_tensor(lam):\n",
    "                    lam = torch.tensor(lam)\n",
    "                lam = lam.to(device).float()  # [B]\n",
    "                lam = lam.view(-1, 1)         # [B,1] for broadcasting\n",
    "                \n",
    "                # Random: use MixUp or CutMix\n",
    "                if enable_cutmix and random.random() < 0.5:\n",
    "                    imgs, labels1, labels2, lam_cutmix = cutmix(imgs, labels1, CONFIG['cutmix_alpha'])\n",
    "                    # CutMix returns scalar lam, convert to per-sample\n",
    "                    lam = torch.full((imgs.size(0), 1), float(lam_cutmix), device=device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                with amp_ctx:\n",
    "                    outputs = model(imgs)\n",
    "                    # Per-sample loss calculation\n",
    "                    if CONFIG['use_label_smoothing']:\n",
    "                        # For label smoothing, we need per-sample loss\n",
    "                        loss1 = F.cross_entropy(outputs, labels1, reduction='none')  # [B]\n",
    "                        loss2 = F.cross_entropy(outputs, labels2, reduction='none')  # [B]\n",
    "                        loss = (lam.squeeze(1) * loss1 + (1-lam.squeeze(1)) * loss2).mean()\n",
    "                    else:\n",
    "                        loss1 = F.cross_entropy(outputs, labels1, reduction='none')  # [B]\n",
    "                        loss2 = F.cross_entropy(outputs, labels2, reduction='none')  # [B]\n",
    "                        loss = (lam.squeeze(1) * loss1 + (1-lam.squeeze(1)) * loss2).mean()\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                train_loss += loss.item() * imgs.size(0)\n",
    "                # Per-sample accuracy estimation\n",
    "                pred = outputs.argmax(1)\n",
    "                train_correct += (lam.squeeze(1) * (pred == labels1).float() + \n",
    "                                (1-lam.squeeze(1)) * (pred == labels2).float()).sum().item()\n",
    "                train_total += imgs.size(0)\n",
    "            else:\n",
    "                imgs, labels = batch if len(batch) == 2 else (batch[0], batch[1])\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "                \n",
    "                # Apply CutMix randomly (only if strong augmentation enabled)\n",
    "                if enable_cutmix and random.random() < 0.5:\n",
    "                    imgs, labels_a, labels_b, lam = cutmix(imgs, labels, CONFIG['cutmix_alpha'])\n",
    "                    lam = torch.tensor(float(lam), device=device, dtype=torch.float32)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    with amp_ctx:\n",
    "                        outputs = model(imgs)\n",
    "                        # Use consistent loss function\n",
    "                        if CONFIG['use_label_smoothing']:\n",
    "                            loss1 = F.cross_entropy(outputs, labels_a, reduction='none')\n",
    "                            loss2 = F.cross_entropy(outputs, labels_b, reduction='none')\n",
    "                            loss = (lam * loss1 + (1-lam) * loss2).mean()\n",
    "                        else:\n",
    "                            loss1 = F.cross_entropy(outputs, labels_a, reduction='none')\n",
    "                            loss2 = F.cross_entropy(outputs, labels_b, reduction='none')\n",
    "                            loss = (lam * loss1 + (1-lam) * loss2).mean()\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    train_loss += loss.item() * imgs.size(0)\n",
    "                    train_correct += (lam * (outputs.argmax(1) == labels_a).float() + \n",
    "                                    (1-lam) * (outputs.argmax(1) == labels_b).float()).sum().item()\n",
    "                    train_total += imgs.size(0)\n",
    "                else:\n",
    "                    optimizer.zero_grad()\n",
    "                    with amp_ctx:\n",
    "                        outputs = model(imgs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    train_loss += loss.item() * imgs.size(0)\n",
    "                    train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                    train_total += imgs.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "                with amp_ctx:\n",
    "                    outputs = model(imgs)\n",
    "                    # Use standard cross-entropy for validation (no augmentation)\n",
    "                    loss = F.cross_entropy(outputs, labels)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                val_total += imgs.size(0)\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        \n",
    "        logging.info(f\"Epoch {epoch}: TL={train_loss:.4f} TA={train_acc:.4f} VL={val_loss:.4f} VA={val_acc:.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            logging.info(f\"  → Best model saved (Val Acc: {val_acc:.4f})\")\n",
    "    \n",
    "    return history, best_path\n",
    "\n",
    "# Train\n",
    "train_crops = crops_df[crops_df['split'] == 'train']\n",
    "val_crops = crops_df[crops_df['split'] == 'val']\n",
    "\n",
    "model = build_enhanced_mobilenetv3(len(LABELS))\n",
    "history, best_checkpoint = train_enhanced_model(model, train_crops, val_crops, CONFIG['epochs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 8: Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_report(history, model_name, output_dir):\n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"CREATING TRAINING REPORT\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # 1. Loss curves\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    ax1.plot(epochs_range, history['train_loss'], 'b-', label='Train', linewidth=2, marker='o', markersize=4)\n",
    "    ax1.plot(epochs_range, history['val_loss'], 'r-', label='Val', linewidth=2, marker='s', markersize=4)\n",
    "    ax1.set_xlabel('Epoch', fontsize=10)\n",
    "    ax1.set_ylabel('Loss', fontsize=10)\n",
    "    ax1.set_title('Training & Validation Loss', fontsize=11, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy curves\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    ax2.plot(epochs_range, history['train_acc'], 'b-', label='Train', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.plot(epochs_range, history['val_acc'], 'r-', label='Val', linewidth=2, marker='s', markersize=4)\n",
    "    ax2.set_xlabel('Epoch', fontsize=10)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=10)\n",
    "    ax2.set_title('Training & Validation Accuracy', fontsize=11, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Loss zoomed\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    best_epoch = np.argmin(history['val_loss']) + 1\n",
    "    start = max(0, best_epoch - 10)\n",
    "    end = min(len(epochs_range), best_epoch + 5)\n",
    "    ax3.plot(epochs_range[start:end], history['train_loss'][start:end], 'b-', label='Train', linewidth=2)\n",
    "    ax3.plot(epochs_range[start:end], history['val_loss'][start:end], 'r-', label='Val', linewidth=2)\n",
    "    ax3.axvline(best_epoch, color='green', linestyle='--', linewidth=2, label=f'Best: {best_epoch}')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.set_title('Loss (Zoomed)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Accuracy improvement\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    train_acc_improvement = np.diff([0] + history['train_acc'])\n",
    "    val_acc_improvement = np.diff([0] + history['val_acc'])\n",
    "    ax4.plot(epochs_range, train_acc_improvement, 'b-', label='Train Δ', linewidth=2)\n",
    "    ax4.plot(epochs_range, val_acc_improvement, 'r-', label='Val Δ', linewidth=2)\n",
    "    ax4.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Accuracy Change')\n",
    "    ax4.set_title('Epoch-to-Epoch Improvement')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Overfitting gap\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    acc_gap = np.array(history['train_acc']) - np.array(history['val_acc'])\n",
    "    ax5.plot(epochs_range, acc_gap, 'purple', label='Acc Gap (Train-Val)', linewidth=2)\n",
    "    ax5.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    ax5.set_xlabel('Epoch')\n",
    "    ax5.set_ylabel('Gap')\n",
    "    ax5.set_title('Overfitting Monitor')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Val trajectory\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    ax6.plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    ax6.set_xlabel('Epoch')\n",
    "    ax6.set_ylabel('Loss')\n",
    "    ax6.set_title('Val Loss Trajectory')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Metrics summary\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    ax7.axis('off')\n",
    "    best_val_loss_epoch = np.argmin(history['val_loss']) + 1\n",
    "    best_val_acc_epoch = np.argmax(history['val_acc']) + 1\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    TRAINING METRICS SUMMARY\n",
    "    ════════════════════════════\n",
    "    \n",
    "    Model: {model_name}\n",
    "    Total Epochs: {len(history['train_loss'])}\n",
    "    \n",
    "    Best Val Loss:\n",
    "      Epoch: {best_val_loss_epoch}\n",
    "      Loss: {min(history['val_loss']):.4f}\n",
    "      Acc:  {history['val_acc'][best_val_loss_epoch-1]:.4f}\n",
    "    \n",
    "    Best Val Accuracy:\n",
    "      Epoch: {best_val_acc_epoch}\n",
    "      Acc:  {max(history['val_acc']):.4f}\n",
    "      Loss: {history['val_loss'][best_val_acc_epoch-1]:.4f}\n",
    "    \n",
    "    Final Epoch:\n",
    "      Train Loss: {history['train_loss'][-1]:.4f}\n",
    "      Train Acc:  {history['train_acc'][-1]:.4f}\n",
    "      Val Loss:   {history['val_loss'][-1]:.4f}\n",
    "      Val Acc:    {history['val_acc'][-1]:.4f}\n",
    "    \n",
    "    Enhancements:\n",
    "      CBAM: {CONFIG['use_cbam']}\n",
    "      Enhanced Head: {CONFIG['use_better_head']}\n",
    "      CutMix: {CONFIG['use_cutmix']}\n",
    "      Progressive: {CONFIG['use_progressive_resize']}\n",
    "    \"\"\"\n",
    "    ax7.text(0.1, 0.5, metrics_text, fontsize=9, family='monospace',\n",
    "            verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "    \n",
    "    # 8. Stability\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    window = 3\n",
    "    if len(history['val_loss']) >= window:\n",
    "        val_loss_rolling_std = pd.Series(history['val_loss']).rolling(window=window).std()\n",
    "        ax8.plot(epochs_range, val_loss_rolling_std, 'orange', linewidth=2)\n",
    "        ax8.set_xlabel('Epoch')\n",
    "        ax8.set_ylabel('Rolling Std')\n",
    "        ax8.set_title(f'Stability (Window={window})')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Final metrics\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    metrics_names = ['Train Acc', 'Val Acc', 'Train Loss', 'Val Loss']\n",
    "    final_values = [history['train_acc'][-1], history['val_acc'][-1],\n",
    "                   history['train_loss'][-1], history['val_loss'][-1]]\n",
    "    colors_bar = ['steelblue', 'coral', 'lightblue', 'salmon']\n",
    "    bars = ax9.barh(metrics_names, final_values, color=colors_bar)\n",
    "    ax9.set_xlabel('Value')\n",
    "    ax9.set_title('Final Epoch Metrics')\n",
    "    ax9.grid(True, alpha=0.3, axis='x')\n",
    "    for bar, value in zip(bars, final_values):\n",
    "        ax9.text(value, bar.get_y() + bar.get_height()/2, f' {value:.4f}',\n",
    "                va='center', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Training Report', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(output_dir, 'training_report.png')\n",
    "    plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logging.info(f\"✓ Training report saved: {save_path}\")\n",
    "\n",
    "create_training_report(history, \"Enhanced_MobileNetV3_Small\", OUTPUT_DIRS[\"plots\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 9: Test with TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_tta(model, test_df, device):\n",
    "    \"\"\"Test with Test-Time Augmentation\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # TTA transforms\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "            transforms.RandomRotation((90, 90)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "            transforms.RandomRotation((180, 180)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "            transforms.RandomRotation((270, 270)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    ]\n",
    "    \n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Testing with TTA\"):\n",
    "            img = Image.open(row[\"crop_path\"]).convert(\"RGB\")\n",
    "            label = int(row[\"label_id\"])\n",
    "            \n",
    "            # Apply TTA\n",
    "            outputs_list = []\n",
    "            for transform in tta_transforms[:CONFIG['tta_transforms']]:\n",
    "                img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                output = model(img_tensor)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                outputs_list.append(probs)\n",
    "            \n",
    "            # Average predictions\n",
    "            avg_probs = torch.stack(outputs_list).mean(0)\n",
    "            pred = avg_probs.argmax(1)\n",
    "            \n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(label)\n",
    "    \n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.array(all_labels)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1, \n",
    "            'confusion_matrix': cm, 'y_true': y_true, 'y_pred': y_pred}\n",
    "\n",
    "# Test\n",
    "test_crops = crops_df[crops_df['split'] == 'test']\n",
    "model.load_state_dict(torch.load(best_checkpoint, map_location=DEVICE))\n",
    "model = model.to(DEVICE)  # Ensure model is on correct device\n",
    "\n",
    "if CONFIG['use_tta']:\n",
    "    logging.info(f\"\\nTesting with TTA ({CONFIG['tta_transforms']} augmentations)\")\n",
    "    test_metrics = test_with_tta(model, test_crops, DEVICE)\n",
    "else:\n",
    "    # Standard test\n",
    "    _, val_transform = get_transforms(CONFIG['img_size'])\n",
    "    test_dataset = CropDataset(test_crops, transform=val_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                            num_workers=4, pin_memory=True)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            all_preds.append(outputs.argmax(1).cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    test_metrics = {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1,\n",
    "                   'confusion_matrix': cm, 'y_true': y_true, 'y_pred': y_pred}\n",
    "\n",
    "logging.info(\"\\n\" + \"=\"*60)\n",
    "logging.info(\"TEST RESULTS\")\n",
    "logging.info(\"=\"*60)\n",
    "logging.info(f\"Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "logging.info(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "logging.info(f\"Recall:    {test_metrics['recall']:.4f}\")\n",
    "logging.info(f\"F1 Score:  {test_metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 10: Test Results Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_results_report(test_metrics, class_names, output_dir):\n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"CREATING TEST RESULTS REPORT\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # 1. Confusion Matrix - Count\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    sns.heatmap(test_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax1, cbar_kws={'label': 'Count'})\n",
    "    ax1.set_xlabel('Predicted', fontsize=10)\n",
    "    ax1.set_ylabel('True', fontsize=10)\n",
    "    ax1.set_title('Confusion Matrix (Count)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 2. Confusion Matrix - Normalized\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    cm_norm = test_metrics['confusion_matrix'].astype('float') / test_metrics['confusion_matrix'].sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='RdYlGn', vmin=0, vmax=1,\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax2, cbar_kws={'label': 'Proportion'})\n",
    "    ax2.set_xlabel('Predicted', fontsize=10)\n",
    "    ax2.set_ylabel('True', fontsize=10)\n",
    "    ax2.set_title('Confusion Matrix (Normalized)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 3. Per-class metrics\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    p, r, f1, support = precision_recall_fscore_support(\n",
    "        test_metrics['y_true'], test_metrics['y_pred'], labels=range(len(class_names))\n",
    "    )\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.2\n",
    "    ax3.bar(x - width, p, width, label='Precision', alpha=0.8, color='steelblue')\n",
    "    ax3.bar(x, r, width, label='Recall', alpha=0.8, color='coral')\n",
    "    ax3.bar(x + width, f1, width, label='F1-Score', alpha=0.8, color='lightgreen')\n",
    "    \n",
    "    ax3.set_xlabel('Disease Class', fontsize=10)\n",
    "    ax3.set_ylabel('Score', fontsize=10)\n",
    "    ax3.set_title('Per-Class Performance Metrics', fontsize=11, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax3.set_ylim(0, 1.1)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Samples per class\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    unique, counts = np.unique(test_metrics['y_true'], return_counts=True)\n",
    "    class_names_ordered = [class_names[i] for i in unique]\n",
    "    ax4.barh(class_names_ordered, counts, color='mediumpurple')\n",
    "    ax4.set_xlabel('Number of Samples', fontsize=10)\n",
    "    ax4.set_title('Test Set Class Distribution', fontsize=11, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    for i, v in enumerate(counts):\n",
    "        ax4.text(v, i, f' {v}', va='center', fontweight='bold')\n",
    "    \n",
    "    # 5. Accuracy per class\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    class_accuracies = []\n",
    "    for i in range(len(class_names)):\n",
    "        mask = test_metrics['y_true'] == i\n",
    "        if mask.sum() > 0:\n",
    "            acc = (test_metrics['y_pred'][mask] == i).sum() / mask.sum()\n",
    "            class_accuracies.append(acc)\n",
    "        else:\n",
    "            class_accuracies.append(0)\n",
    "    \n",
    "    colors_acc = ['green' if acc >= 0.8 else 'orange' if acc >= 0.6 else 'red' for acc in class_accuracies]\n",
    "    bars = ax5.barh(class_names, class_accuracies, color=colors_acc)\n",
    "    ax5.set_xlabel('Accuracy', fontsize=10)\n",
    "    ax5.set_title('Per-Class Accuracy', fontsize=11, fontweight='bold')\n",
    "    ax5.set_xlim(0, 1.1)\n",
    "    ax5.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for bar, acc in zip(bars, class_accuracies):\n",
    "        ax5.text(acc, bar.get_y() + bar.get_height()/2, f' {acc:.1%}',\n",
    "                va='center', fontweight='bold')\n",
    "    \n",
    "    # 6. Summary statistics\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    TEST SET RESULTS SUMMARY\n",
    "    ════════════════════════════════\n",
    "    \n",
    "    Overall Metrics:\n",
    "      • Accuracy:  {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\n",
    "      • Precision: {test_metrics['precision']:.4f}\n",
    "      • Recall:    {test_metrics['recall']:.4f}\n",
    "      • F1-Score:  {test_metrics['f1']:.4f}\n",
    "    \n",
    "    Total Samples: {len(test_metrics['y_true'])}\n",
    "    Correct:       {(test_metrics['y_true'] == test_metrics['y_pred']).sum()}\n",
    "    Incorrect:     {(test_metrics['y_true'] != test_metrics['y_pred']).sum()}\n",
    "    \n",
    "    Per-Class Accuracy:\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, (name, acc) in enumerate(zip(class_names, class_accuracies)):\n",
    "        summary_text += f\"\\n    {name:15s}: {acc:.4f}\"\n",
    "    \n",
    "    summary_text += f\"\"\"\n",
    "    \n",
    "    Best Class:  {class_names[np.argmax(class_accuracies)]} ({max(class_accuracies):.1%})\n",
    "    Worst Class: {class_names[np.argmin(class_accuracies)]} ({min(class_accuracies):.1%})\n",
    "    \n",
    "    Enhancements Used:\n",
    "      CBAM: {CONFIG['use_cbam']}\n",
    "      Enhanced Head: {CONFIG['use_better_head']}\n",
    "      CutMix: {CONFIG['use_cutmix']}\n",
    "      TTA: {CONFIG['use_tta']} ({CONFIG['tta_transforms']} aug)\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.5, summary_text, fontsize=9, family='monospace',\n",
    "            verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.3))\n",
    "    \n",
    "    plt.suptitle('Comprehensive Test Results Report', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(output_dir, \"test_results_report.png\")\n",
    "    plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logging.info(f\"✓ Test results report saved: {save_path}\")\n",
    "\n",
    "class_names = [LABELS[i]['name'] for i in sorted(LABELS.keys())]\n",
    "create_test_results_report(test_metrics, class_names, OUTPUT_DIRS[\"plots\"])\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_metrics['y_true'], test_metrics['y_pred'], \n",
    "                               target_names=class_names, digits=4)\n",
    "report_path = os.path.join(OUTPUT_DIRS[\"results\"], 'test_classification_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"Enhanced MobileNetV3-Small Test Results\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\\n\\n\")\n",
    "    f.write(\"Enhancements:\\n\")\n",
    "    f.write(f\"  CBAM: {CONFIG['use_cbam']}\\n\")\n",
    "    f.write(f\"  Enhanced Head: {CONFIG['use_better_head']}\\n\")\n",
    "    f.write(f\"  CutMix: {CONFIG['use_cutmix']}\\n\")\n",
    "    f.write(f\"  Progressive Resize: {CONFIG['use_progressive_resize']}\\n\")\n",
    "    f.write(f\"  TTA: {CONFIG['use_tta']} ({CONFIG['tta_transforms']} transforms)\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 11: Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_production(model, model_name, save_dir, example_input):\n",
    "    model.eval()\n",
    "    \n",
    "    # TorchScript\n",
    "    try:\n",
    "        traced = torch.jit.trace(model, example_input)\n",
    "        traced_path = os.path.join(save_dir, f'{model_name}_traced.pt')\n",
    "        torch.jit.save(traced, traced_path)\n",
    "        logging.info(f\"✓ Exported TorchScript: {traced_path}\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"TorchScript export failed: {e}\")\n",
    "    \n",
    "    # ONNX\n",
    "    try:\n",
    "        onnx_path = os.path.join(save_dir, f'{model_name}.onnx')\n",
    "        torch.onnx.export(model, example_input, onnx_path,\n",
    "                         input_names=['input'], output_names=['output'],\n",
    "                         dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "                         opset_version=11)\n",
    "        logging.info(f\"✓ Exported ONNX: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"ONNX export failed: {e}\")\n",
    "\n",
    "example_input = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).to(DEVICE)\n",
    "export_for_production(model, \"Enhanced_MobileNetV3_Small\", OUTPUT_DIRS[\"exports\"], example_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 12: Inference Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRiceFieldAnalyzer:\n",
    "    def __init__(self, yolo_model_path, classification_model_path, device=DEVICE):\n",
    "        self.device = device\n",
    "        self.yolo_model = YOLO(yolo_model_path)\n",
    "        \n",
    "        self.classifier = build_enhanced_mobilenetv3(len(LABELS))\n",
    "        self.classifier.load_state_dict(torch.load(classification_model_path, map_location=device))\n",
    "        self.classifier.to(device)\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        _, self.transform = get_transforms(CONFIG['img_size'])\n",
    "        self.class_names = [LABELS[i]['name'] for i in sorted(LABELS.keys())]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def analyze_field_image(self, image_path, yolo_conf=0.3, use_tta=False):\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        \n",
    "        yolo_results = self.yolo_model.predict(image_path, conf=yolo_conf, verbose=False)\n",
    "        \n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Cannot read: {image_path}\")\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for result in yolo_results[0].boxes:\n",
    "            x1, y1, x2, y2 = map(int, result.xyxy[0].cpu().numpy())\n",
    "            crop = img_rgb[y1:y2, x1:x2]\n",
    "            \n",
    "            if crop.size == 0 or crop.shape[0] < 50 or crop.shape[1] < 50:\n",
    "                continue\n",
    "            \n",
    "            crop_pil = Image.fromarray(crop)\n",
    "            \n",
    "            if use_tta and CONFIG['use_tta']:\n",
    "                # TTA inference\n",
    "                tta_transforms = [\n",
    "                    self.transform,\n",
    "                    transforms.Compose([\n",
    "                        transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "                        transforms.RandomHorizontalFlip(p=1.0),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "                ]\n",
    "                \n",
    "                outputs_list = []\n",
    "                for transform in tta_transforms:\n",
    "                    crop_tensor = transform(crop_pil).unsqueeze(0).to(self.device)\n",
    "                    output = self.classifier(crop_tensor)\n",
    "                    probs = F.softmax(output, dim=1)\n",
    "                    outputs_list.append(probs)\n",
    "                \n",
    "                avg_probs = torch.stack(outputs_list).mean(0)\n",
    "                conf, pred = torch.max(avg_probs, 1)\n",
    "            else:\n",
    "                crop_tensor = self.transform(crop_pil).unsqueeze(0).to(self.device)\n",
    "                output = self.classifier(crop_tensor)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                conf, pred = torch.max(probs, 1)\n",
    "            \n",
    "            pred_class = self.class_names[pred.item()]\n",
    "            conf_score = conf.item()\n",
    "            \n",
    "            predictions.append({\n",
    "                'class': pred_class,\n",
    "                'confidence': conf_score,\n",
    "                'bbox': (x1, y1, x2, y2)\n",
    "            })\n",
    "        \n",
    "        return predictions, img_rgb\n",
    "    \n",
    "    def visualize_results(self, image_path, predictions, img_rgb, save_path=None):\n",
    "        if img_rgb is None or len(predictions) == 0:\n",
    "            print(\"No predictions\")\n",
    "            return\n",
    "        \n",
    "        img_draw = img_rgb.copy()\n",
    "        \n",
    "        for pred in predictions:\n",
    "            x1, y1, x2, y2 = pred['bbox']\n",
    "            color = (0, 255, 0) if pred['class'] == 'healthy' else (255, 0, 0)\n",
    "            cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, 3)\n",
    "            \n",
    "            label = f\"{pred['class']}: {pred['confidence']:.2f}\"\n",
    "            cv2.putText(img_draw, label, (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(img_draw)\n",
    "        plt.title(f'Detection: {len(predictions)} leaves', fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary\n",
    "        disease_counts = {}\n",
    "        for p in predictions:\n",
    "            disease_counts[p['class']] = disease_counts.get(p['class'], 0) + 1\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total leaves: {len(predictions)}\")\n",
    "        for disease, count in sorted(disease_counts.items()):\n",
    "            pct = count/len(predictions)*100\n",
    "            print(f\"  {disease:15s}: {count:3d} ({pct:5.1f}%)\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "analyzer = EnhancedRiceFieldAnalyzer(\n",
    "    yolo_model_path=best_yolo_model,\n",
    "    classification_model_path=best_checkpoint,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "logging.info(\"\\n✓ Enhanced Analyzer ready for inference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PHASE 13: Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = field_data['test']\n",
    "samples = random.sample(test_images, min(3, len(test_images)))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    image_path = sample['image_path']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Image {i+1}\")\n",
    "    print(f\"True label: {sample['label_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    predictions, img_rgb = analyzer.analyze_field_image(image_path, yolo_conf=0.3, use_tta=False)\n",
    "    save_path = os.path.join(OUTPUT_DIRS[\"demo\"], f\"demo_{i+1}.png\")\n",
    "    analyzer.visualize_results(image_path, predictions, img_rgb, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\\n\" + \"=\"*80)\n",
    "logging.info(\"COMPLETE ENHANCED PIPELINE FINISHED\")\n",
    "logging.info(\"=\"*80)\n",
    "logging.info(f\"Output: {PATH_OUTPUT}\")\n",
    "logging.info(f\"\\nModel: Enhanced MobileNetV3-Small\")\n",
    "logging.info(f\"\\nEnhancements Applied:\")\n",
    "if CONFIG['use_cbam']:\n",
    "    logging.info(\"  ✓ CBAM Attention Module\")\n",
    "if CONFIG['use_better_head']:\n",
    "    logging.info(\"  ✓ Enhanced Classification Head (Dual Pooling)\")\n",
    "if CONFIG['use_cutmix']:\n",
    "    logging.info(\"  ✓ CutMix Augmentation\")\n",
    "if CONFIG['use_mixup']:\n",
    "    logging.info(\"  ✓ MixUp Augmentation\")\n",
    "if CONFIG['use_progressive_resize']:\n",
    "    logging.info(\"  ✓ Progressive Resizing\")\n",
    "if CONFIG['use_label_smoothing']:\n",
    "    logging.info(\"  ✓ Label Smoothing\")\n",
    "if CONFIG['use_tta']:\n",
    "    logging.info(f\"  ✓ Test-Time Augmentation ({CONFIG['tta_transforms']} transforms)\")\n",
    "\n",
    "logging.info(f\"\\nFinal Test Results:\")\n",
    "logging.info(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "logging.info(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "logging.info(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "logging.info(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "\n",
    "logging.info(f\"\\nAll Visualizations:\")\n",
    "logging.info(\"  ✓ Crop extraction samples (6 samples)\")\n",
    "logging.info(\"  ✓ Data analysis report (9 charts)\")\n",
    "logging.info(\"  ✓ Training report (9 charts)\")\n",
    "logging.info(\"  ✓ Test results report (6 visualizations)\")\n",
    "logging.info(\"  ✓ Demo outputs\")\n",
    "\n",
    "logging.info(f\"\\nExports:\")\n",
    "logging.info(f\"  ✓ Best model: {best_checkpoint}\")\n",
    "logging.info(f\"  ✓ TorchScript & ONNX in: {OUTPUT_DIRS['exports']}\")\n",
    "\n",
    "logging.info(\"=\"*80)\n",
    "logging.info(\"Ready for production deployment!\")\n",
    "logging.info(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
