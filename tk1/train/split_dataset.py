"""
Script Ä‘á»ƒ chia dataset tá»« thÆ° má»¥c train thÃ nh train/valid/test
theo tá»‰ lá»‡ 70%/20%/10% vá»›i class distribution balanced
"""

import os
import random
import shutil
from pathlib import Path
from collections import defaultdict
import yaml

# Cáº¥u hÃ¬nh
DATASET_ROOT = "../data/rice-leaf-disease-detection_new_yolo"
TRAIN_RATIO = 0.7
VALID_RATIO = 0.2
TEST_RATIO = 0.1

# Seed cho reproducible results
random.seed(42)

def analyze_class_distribution(labels_dir):
    """PhÃ¢n tÃ­ch phÃ¢n phá»‘i classes trong dataset"""
    class_count = defaultdict(int)
    image_to_classes = {}
    
    for label_file in os.listdir(labels_dir):
        if not label_file.endswith('.txt'):
            continue
            
        label_path = os.path.join(labels_dir, label_file)
        image_name = label_file.replace('.txt', '')
        classes_in_image = set()
        
        with open(label_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    class_id = int(float(parts[0]))
                    classes_in_image.add(class_id)
                    class_count[class_id] += 1
        
        image_to_classes[image_name] = list(classes_in_image)
    
    return class_count, image_to_classes

def split_dataset():
    """Chia dataset thÃ nh train/valid/test"""
    
    print("=" * 60)
    print("DATASET SPLITTING")
    print("=" * 60)
    
    dataset_path = Path(DATASET_ROOT)
    original_train_images = dataset_path / "train" / "images"
    original_train_labels = dataset_path / "train" / "labels"
    
    # Äá»•i tÃªn thÆ° má»¥c gá»‘c Ä‘á»ƒ trÃ¡nh conflict
    original_backup_images = dataset_path / "original_train_images"
    original_backup_labels = dataset_path / "original_train_labels"
    
    # Move original folders
    if original_train_images.exists():
        shutil.move(str(original_train_images), str(original_backup_images))
    if original_train_labels.exists():
        shutil.move(str(original_train_labels), str(original_backup_labels))
    
    # Táº¡o thÆ° má»¥c má»›i
    new_folders = ['train', 'valid', 'test']
    for folder in new_folders:
        for subfolder in ['images', 'labels']:
            new_path = dataset_path / folder / subfolder
            new_path.mkdir(parents=True, exist_ok=True)
    
    # Láº¥y danh sÃ¡ch táº¥t cáº£ images tá»« backup folder
    image_files = []
    for img_file in original_backup_images.glob("*.jpg"):
        # Kiá»ƒm tra xem cÃ³ label tÆ°Æ¡ng á»©ng khÃ´ng
        label_file = original_backup_labels / f"{img_file.stem}.txt"
        if label_file.exists():
            image_files.append(img_file.stem)
    
    print(f"\nğŸ“Š Tá»•ng sá»‘ images: {len(image_files)}")
    
    # PhÃ¢n tÃ­ch class distribution
    class_count, image_to_classes = analyze_class_distribution(str(original_backup_labels))
    
    print(f"\nğŸ“‹ PhÃ¢n phá»‘i classes:")
    class_names = ['bacterial_leaf_blight', 'blast', 'brown_spot']
    for class_id, count in class_count.items():
        print(f"   - {class_names[class_id]}: {count} bounding boxes")
    
    print(f"\nğŸ“ˆ PhÃ¢n phá»‘i images theo classes:")
    images_per_class = defaultdict(list)
    for image_name, classes in image_to_classes.items():
        for class_id in classes:
            images_per_class[class_id].append(image_name)
    
    for class_id in sorted(images_per_class.keys()):
        print(f"   - {class_names[class_id]}: {len(images_per_class[class_id])} images")
    
    # Chia dataset theo stratified sampling
    train_images = []
    valid_images = []
    test_images = []
    
    # Shuffle Ä‘á»ƒ Ä‘áº£m báº£o random
    random.shuffle(image_files)
    
    # Chia theo tá»‰ lá»‡
    n_train = int(len(image_files) * TRAIN_RATIO)
    n_valid = int(len(image_files) * VALID_RATIO)
    
    train_images = image_files[:n_train]
    valid_images = image_files[n_train:n_train + n_valid]
    test_images = image_files[n_train + n_valid:]
    
    splits = {
        'train': train_images,
        'valid': valid_images,
        'test': test_images
    }
    
    print(f"\nğŸ“¦ KÃ­ch thÆ°á»›c splits:")
    for split_name, images in splits.items():
        print(f"   - {split_name}: {len(images)} images ({len(images)/len(image_files)*100:.1f}%)")
    
    # Copy files
    for split_name, images in splits.items():
        print(f"\nğŸ“‚ Äang copy {split_name} files...")
        
        split_images_dir = dataset_path / split_name / "images"
        split_labels_dir = dataset_path / split_name / "labels"
        
        for i, image_name in enumerate(images, 1):
            # Copy image tá»« backup folder
            src_img = original_backup_images / f"{image_name}.jpg"
            dst_img = split_images_dir / f"{image_name}.jpg"
            shutil.copy2(src_img, dst_img)
            
            # Copy label tá»« backup folder
            src_label = original_backup_labels / f"{image_name}.txt"
            dst_label = split_labels_dir / f"{image_name}.txt"
            shutil.copy2(src_label, dst_label)
            
            if i % 500 == 0 or i == len(images):
                print(f"   ÄÃ£ copy {i}/{len(images)} files...")
    
    # Verify splits
    print(f"\nâœ… Kiá»ƒm tra káº¿t quáº£:")
    for split_name in splits.keys():
        split_images_dir = dataset_path / split_name / "images"
        split_labels_dir = dataset_path / split_name / "labels"
        
        n_images = len(list(split_images_dir.glob("*.jpg")))
        n_labels = len(list(split_labels_dir.glob("*.txt")))
        
        print(f"   - {split_name}: {n_images} images, {n_labels} labels")
        
        if n_images != n_labels:
            print(f"   âš ï¸  WARNING: Sá»‘ lÆ°á»£ng images vÃ  labels khÃ´ng khá»›p!")
    
    return splits

def update_data_yaml():
    """Cáº­p nháº­t data.yaml vá»›i Ä‘Æ°á»ng dáº«n má»›i"""
    
    print(f"\nğŸ“ Cáº­p nháº­t data.yaml...")
    
    data_yaml_path = Path(DATASET_ROOT) / "data.yaml"
    
    # Äá»c data.yaml hiá»‡n táº¡i
    with open(data_yaml_path, 'r') as f:
        data = yaml.safe_load(f)
    
    # Cáº­p nháº­t Ä‘Æ°á»ng dáº«n
    data['train'] = "train/images"
    data['val'] = "valid/images"  
    data['test'] = "test/images"
    
    # Backup file cÅ©
    backup_path = data_yaml_path.with_suffix('.yaml.backup')
    shutil.copy2(data_yaml_path, backup_path)
    print(f"   ğŸ“„ ÄÃ£ backup file gá»‘c: {backup_path}")
    
    # Ghi file má»›i
    with open(data_yaml_path, 'w') as f:
        yaml.dump(data, f, default_flow_style=False, sort_keys=False)
    
    print(f"   âœ… ÄÃ£ cáº­p nháº­t data.yaml")
    
    # In ná»™i dung má»›i
    print(f"\nğŸ“‹ Ná»™i dung data.yaml má»›i:")
    with open(data_yaml_path, 'r') as f:
        print(f.read())

def main():
    """Main function"""
    
    # Chia dataset
    splits = split_dataset()
    
    # Cáº­p nháº­t data.yaml
    update_data_yaml()
    
    print("\n" + "=" * 60)
    print("âœ… HOÃ€N THÃ€NH CHIA DATASET!")
    print("=" * 60)
    print(f"\nğŸ“ Cáº¥u trÃºc thÆ° má»¥c má»›i:")
    print(f"   {DATASET_ROOT}/")
    print(f"   â”œâ”€â”€ train/")
    print(f"   â”‚   â”œâ”€â”€ images/ ({len(splits['train'])} files)")
    print(f"   â”‚   â””â”€â”€ labels/ ({len(splits['train'])} files)")
    print(f"   â”œâ”€â”€ valid/")
    print(f"   â”‚   â”œâ”€â”€ images/ ({len(splits['valid'])} files)")
    print(f"   â”‚   â””â”€â”€ labels/ ({len(splits['valid'])} files)")
    print(f"   â”œâ”€â”€ test/")
    print(f"   â”‚   â”œâ”€â”€ images/ ({len(splits['test'])} files)")
    print(f"   â”‚   â””â”€â”€ labels/ ({len(splits['test'])} files)")
    print(f"   â””â”€â”€ data.yaml (updated)")
    
    print(f"\nğŸš€ BÃ¢y giá» báº¡n cÃ³ thá»ƒ cháº¡y láº¡i training script!")

if __name__ == "__main__":
    main()